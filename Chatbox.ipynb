{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Necessary packages:\n",
    "```\n",
    "langchain_huggingface\n",
    "langchain\n",
    "transformers\n",
    "accelerate\n",
    "ipywidget (for jupyter notebook)\n",
    "\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126 (for gpu inference, my cuda version is 12.6, so with suffic cu126)\n",
    "```\n",
    "\n",
    "Notice that we need to download the weights of the deepseek model from HuggingFace for further process. (in total about 20GB)\n",
    "\n",
    "     "
   ],
   "id": "53dd465e61de70dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:19:34.514988Z",
     "start_time": "2025-04-23T20:19:31.299084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check GPU availability\n",
    "import torch\n",
    "print(torch.cuda.is_available(), torch.cuda.get_device_name(0))"
   ],
   "id": "cb14869d5196fc8b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:22:06.695521Z",
     "start_time": "2025-04-23T20:20:46.422620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline    # wrap the local model in HuggingFace chat wrapper\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline    # transformers allows us to access checkpoints of pretrained models\n",
    "\n",
    "model_id = \"deepseek-ai/deepseek-coder-6.7b-instruct\"                   # deepseek model which is good at coding\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             torch_dtype = \"auto\",      # torch.float16, precision of weights\n",
    "                                             device_map = \"auto\",       # assign tasks to both GPU and CPU if GPU memory is not enough\n",
    "                                             trust_remote_code = True)  # DeepSeek ships custom model class; must be allowed  \n",
    "\n",
    "pipe = pipeline(task = \"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer = tokenizer,\n",
    "                max_new_tokens = 512,\n",
    "                do_sample = False)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline = pipe)\n",
    "# call the deepseek model\n",
    "\n",
    "deepseek = ChatHuggingFace(llm = llm)\n",
    "resp = deepseek.invoke(\"Explain the Doppler effect in two sentences.\")\n",
    "print(resp.content)           "
   ],
   "id": "1aafc5d73c3d6e9c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6d5118a064b4a3da6fdb40f1b31793c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\n",
      "### Instruction:\n",
      "Explain the Doppler effect in two sentences.\n",
      "### Response:\n",
      "I'm sorry, but as an AI programming assistant, I specialize in computer science and programming-related questions. The Doppler effect is a topic related to physics, which is outside of my expertise. I recommend seeking information from a source or an expert that specializes in physics for a comprehensive explanation.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:25:44.823814Z",
     "start_time": "2025-04-23T20:25:02.014036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "resp = deepseek.invoke(\"Explain the Greedy Alrogithm in two sentences.\")\n",
    "print(resp.content)      "
   ],
   "id": "4ff7973f33e558e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\n",
      "### Instruction:\n",
      "Explain the Greedy Alrogithm in two sentences.\n",
      "### Response:\n",
      "A Greedy Algorithm is an algorithmic paradigm that follows the problem-solving heuristic of making the locally optimal choice at each stage with the hope that these local optimums will lead to a global optimum.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9c6912f2ad4ede14"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
